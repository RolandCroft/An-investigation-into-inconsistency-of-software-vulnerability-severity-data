# Predict bug severity based off description
import time
import re
import itertools
import pandas as pd
import numpy as np
import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction import text
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef
# Just use count vectorizer (from 'On the unreliability of bug severity data')
from sklearn.feature_extraction.text import CountVectorizer
# LR, SVM, RF, XGB (From Triet's work) (Exclude NB and LGBM for redundancy)
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC


# ML models and hyperparameters to tune
model_parameters = {'lr': ['0.01', '0.1', '1', '10', '100'],
                    'svm': ['0.1', '1', '10', '100', '1000', '10000'],
                    'rf':  [['100', '300', '500'], ['100', '200', '300']]}

# Define stop word list
my_stop_words = text.ENGLISH_STOP_WORDS
my_stop_words = list(my_stop_words) + list(stopwords.words('english'))
my_stop_words = list(set(my_stop_words))

# the appropriate columns to use
date_columns = {'br': 'Bug Open Date', 'advisory': 'Announced',
                'nvd': 'NVD Publish Date'}
desc_columns = {'br': 'Bug Report Description Clean',
                'advisory': 'Advisory Description Clean',
                'nvd': 'NVD Description Clean'}
label_columns = {'br': 'Norm BR Severity',
                 'advisory': 'Norm Advisory Severity',
                 'nvd': 'Norm CVSS2 Severity'}


# Text preprocessing module
def preprocess_text(s):
    s = str(s)
    if len(s) < 2:
        return ''

    # Remove any remaining html snippets
    p = re.compile(r'<.*?>')
    s = p.sub('', s)
    # Remove punctuation
    s = s.replace('\'', '').replace(', ', ' ').replace('; ', ' ').\
        replace('. ', ' ').replace('(', '').replace(')', '').strip().lower()
    if not s[-1].isalnum():
        s = s[:-1]
    # Tokenize
    words = s.split()
    # Stemming and stop word removal
    ps = PorterStemmer()
    words = [ps.stem(w) for w in words if w not in my_stop_words]
    # Remove numbers. Check if first char is number to also remove addresses.
    words = [i for i in words if not i[0].isnumeric()]

    return ' '.join(words)


# Clean the description
def clean_descriptions(df):
    df['Bug Report Description Clean'] = df['Bug Report Description'].apply(
        lambda x: preprocess_text(x))
    df['Advisory Description Clean'] = df['Description'].apply(
        lambda x: preprocess_text(x))
    df['NVD Description Clean'] = df['NVD Description'].apply(
        lambda x: preprocess_text(x))
    df = df.drop(columns=['Bug Report Description', 'Description', 'NVD Description'])
    return df


# Train and validate a machine learning classifier
def train_classifier(x_train, y_train, x_val, y_val):
    # Store validation results
    best_clf, best_mcc, best_params = -1, -1, -1
    # Get the model
    for model in model_parameters:
        # Get parameter list
        if model == 'lr' or model == 'svm':
            param_list = list(itertools.product(*[model_parameters[model]]))
        else:
            param_list = list(itertools.product(*model_parameters[model]))
        for parameters in param_list:
            if model == 'lr':
                clf = LogisticRegression(C=float(parameters[0]),
                                         multi_class='multinomial',
                                         solver='lbfgs', tol=0.001,
                                         max_iter=1000, random_state=42)
            elif model == 'svm':
                clf = SVC(max_iter=-1, C=float(parameters[0]), kernel='rbf',
                          random_state=42, probability=True)
            elif model == 'rf':
                clf = RandomForestClassifier(n_estimators=int(parameters[0]),
                                             max_depth=None,
                                             max_leaf_nodes=int(parameters[1]),
                                             random_state=42)
            elif model == 'xgb':
                clf = XGBClassifier(objective='reg:squarederror', max_depth=0,
                                    n_estimators=int(parameters[0]),
                                    max_leaves=int(parameters[1]),
                                    grow_policy='lossguide', random_state=42,
                                    tree_method='hist')
            # Train the classifier
            clf.fit(x_train, y_train)
            # Evaluate the classifier
            y_pred = clf.predict(x_val)
            precision, recall, f1, mcc = evaluate(y_pred, y_val)
            # Store the best performing parameters
            if mcc > best_mcc:
                best_mcc = mcc
                best_clf = clf
                best_params = '_'.join([model, '_'.join(parameters), str(mcc)])
    return best_clf, best_params


# Evaluate prediction performance
def evaluate(y_pred, y_test):
    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
    mcc = matthews_corrcoef(y_test, y_pred)
    return precision, recall, f1, mcc


def main(desc_set, label_set):
    # Load the dataset
    df = pd.read_csv('data/processed_bug_report_data.csv')
    # Sort the dataset by date
    df = df.sort_values(by=[date_columns[desc_set]]).reset_index(drop=True)
    # Clean the description
    df = clean_descriptions(df)
    # Partition the data
    train = df[:int(0.8*len(df))]
    val = df[int(0.8*len(df)):int(0.9*len(df))]
    test = df[int(0.9*len(df)):]
    # Extract features
    vectorizer = CountVectorizer(min_df=0.001)  # Use 0.1% cutoff as per Triet's work
    x_train = vectorizer.fit_transform(train[desc_columns[desc_set]])
    x_val = vectorizer.transform(val[desc_columns[desc_set]])
    x_test = vectorizer.transform(test[desc_columns[desc_set]])
    vocab = vectorizer.vocabulary_
    # Extract labels
    y_train = train[label_columns[label_set]]
    y_val = val[label_columns[label_set]]
    y_test = test[label_columns[label_set]]
    # Train the model
    clf, clf_params = train_classifier(x_train, y_train, x_val, y_val)
    # Evaluate
    y_pred = clf.predict(x_test)
    precision, recall, f1, mcc = evaluate(y_pred, y_test)
    # Print outputs
    print('Use Case: ', desc_set, label_set)
    print('Classifier: ', clf_params)
    print('Precision: ', precision)
    print('Recall: ', recall)
    print('F1: ', f1)
    print('MCC: ', mcc)
    print('-'*10)


if __name__ == '__main__':
    for i in list(itertools.product(['br', 'nvd', 'advisory'], ['br', 'nvd', 'advisory'])):
        main(i[0], i[1])
    exit()
